{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "does_it_for_submission = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyternotify\n",
    "\n",
    "%store -r item_cat\n",
    "%store -r item\n",
    "%store -r shops\n",
    "%store -r sales_train\n",
    "%store -r train\n",
    "%store -r train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.realpath('./../../scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksymsuprunenko/anaconda3/lib/python3.7/site-packages/tqdm/_tqdm.py:604: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Data Analysis tools was loaded\n"
     ]
    }
   ],
   "source": [
    "__da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maksymsuprunenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import googlemaps\n",
    "import plotly.express as px\n",
    "from functools import partial\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, KFold\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# TSFRESH\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, extract_features, MinimalFCParameters, EfficientFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import extract_relevant_features\n",
    "\n",
    "# Sklearn-pandas\n",
    "from sklearn_pandas import CategoricalImputer, FunctionTransformer, DataFrameMapper\n",
    "\n",
    "# SCIPY\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# My files\n",
    "from basic_text_preprocessing import BasicPreprocessText\n",
    "\n",
    "gmaps = googlemaps.Client(key='AIzaSyCW4PTjjIz6yGUgAmqrG2cLy9euzbim23M')\n",
    "\n",
    "from math import cos, asin, sqrt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):    \n",
    "    tmp = df[['date_block_num', 'shop_id','item_id', col]]\n",
    "    for i in tqdm(lags):\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] = shifted['date_block_num'] + i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df\n",
    "\n",
    "def mean_encoding(df, groupby_feats, target, enc, lags):\n",
    "    print('Features: ' , groupby_feats)\n",
    "    features = df[[*groupby_feats, target]]\\\n",
    "             .groupby(groupby_feats, as_index=False)\\\n",
    "             .agg(['mean'])\n",
    "   \n",
    "    features.columns = [enc]\n",
    "    \n",
    "    df = df.merge(features, on=groupby_feats, how='left')\n",
    "    df[enc] = df[enc].astype(np.float16)\n",
    "    df = lag_feature(df, lags, enc).fillna(0)\n",
    "    df.drop(enc, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shop feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295     #Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 12742 * np.asin(sqrt(a))\n",
    "\n",
    "def not_city_str(x, t):\n",
    "    return 1 if t in \"\".join(x.split()[1:]) else 0\n",
    "\n",
    "def get_location(x):\n",
    "    loc = gmaps.geocode(x)\n",
    "    return loc[0]['geometry']['location'] if len(loc) != 0 else {'lat': 0, 'lng': 0}\n",
    "\n",
    "moskov_lat, moskov_lng = get_location('Moscow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_shops = shops.copy()\n",
    "# cleaned_shop_name = BasicPreprocessText().vectorize_process_text(shops['shop_name'])\n",
    "# new_shops['shop_name'] = cleaned_shop_name\n",
    "# new_shops['city'] = new_shops['shop_name'].apply(lambda x: x.split()[0])\n",
    "# city = new_shops['city'] .value_counts()\\\n",
    "# .to_frame().reset_index().rename(columns={'index': 'shop_name', 'city': 'count_shops'})\n",
    "\n",
    "\n",
    "# new_shops['is_mal'] = new_shops['shop_name'].apply(partial(not_city_str, t='тц')).astype(np.int8)\n",
    "# new_shops['is_en_mal'] = new_shops['shop_name'].apply(partial(not_city_str, t='трк')).astype(np.int8)\n",
    "\n",
    "# locations = new_shops['shop_name'].progress_apply(get_location) \n",
    "\n",
    "# new_shops_with_coords = pd.concat([new_shops, pd.DataFrame.from_records(locations.values)], axis=1)\n",
    "\n",
    "# new_shops_with_coords.to_pickle(\"new_shops_with_coords.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shops_with_coords = pd.read_pickle(\"new_shops_with_coords.pickle\")\n",
    "moskov_lat, moskov_lng = list(get_location('Moscow').values())\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295     #Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 12742 * np.arcsin(sqrt(a))\n",
    "\n",
    "new_shops_with_coords['lat'] = new_shops_with_coords['lat'].astype(np.float16, copy=False)\n",
    "new_shops_with_coords['lng'] = new_shops_with_coords['lng'].astype(np.float16, copy=False)\n",
    "\n",
    "new_shops_with_coords['distance_to_moskov'] = \\\n",
    "    new_shops_with_coords[['lat', 'lng']].apply(lambda x: distance(x[0], x[1], moskov_lat, moskov_lng), axis=1)\\\n",
    "    .astype(np.float16)\n",
    "\n",
    "new_shops_with_coords['city'] = LabelEncoder().fit_transform(new_shops_with_coords['city']).astype(np.int8)\n",
    "#new_shops_with_coords['city'] = LabelEncoder().fit_transform(new_shops_with_coords['city']).astype(np.int8)\n",
    "\n",
    "#text_features = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\\\n",
    " #              .fit_transform(new_shops_with_coords['shop_name'])\\\n",
    "  #             .astype(np.float16)\n",
    "    \n",
    "#new_shops_with_coords = pd.concat([new_shops_with_coords, pd.DataFrame.sparse.from_spmatrix(text_features)], axis=1)\n",
    "new_shops_with_coords = new_shops_with_coords.drop('shop_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"daca12af-0e47-4faf-b677-55e4b7c40081\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"daca12af-0e47-4faf-b677-55e4b7c40081\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "item_cleaned = item.copy()\n",
    "le = LabelEncoder().fit(item_cleaned['item_name'])\n",
    "item_cleaned['item_name'] = le.transform(item_cleaned['item_name'])\n",
    "#cleaned_item_name_series = pd.read_pickle(\"cleaned_item_name_series.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item category generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat_cleaned = item_cat.copy()\n",
    "\n",
    "item_category_names = pd.Series(BasicPreprocessText().vectorize_process_text(item_cat_cleaned['item_category_name']))\n",
    "\n",
    "item_cat_cleaned['item_category_names_category_1'] = item_category_names.apply(lambda x: x.split()[0])\n",
    "item_cat_cleaned['item_category_names_category_2'] = item_category_names.apply(lambda x: \" \".join(x.split()[1:]))\n",
    "\n",
    "item_cat_cleaned['item_category_names_category_1_enc'] = \\\n",
    "    LabelEncoder().fit_transform(item_cat_cleaned['item_category_names_category_1']).astype(np.int8)\n",
    "\n",
    "item_cat_cleaned['item_category_names_category_2_enc'] = \\\n",
    "    LabelEncoder().fit_transform(item_cat_cleaned['item_category_names_category_2']).astype(np.int8)\n",
    "\n",
    "item_cat_cleaned = item_cat_cleaned.drop(\n",
    "    ['item_category_name', 'item_category_names_category_1', 'item_category_names_category_2'], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join on everthing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10913849/10913849 [04:02<00:00, 45040.14it/s]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"25011d66-c031-4f0a-a5ac-9dacd85c8374\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"25011d66-c031-4f0a-a5ac-9dacd85c8374\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "\n",
    "predict_month = 34 if does_it_for_submission else 33\n",
    "train_df = train_test.copy() if does_it_for_submission else train.copy()\n",
    "\n",
    "train_df = train_df.merge(item_cleaned[['item_id', 'item_category_id']], how='left', on='item_id', suffixes=(\"\", \"_item\"), right_index=False)\n",
    "train_df = train_df.merge(new_shops_with_coords, how='left', on='shop_id', suffixes=(\"\", \"_shops\"), right_index=False)\n",
    "train_df = train_df.merge(item_cat_cleaned, how='left', on='item_category_id', suffixes=(\"\", \"_item_cat\"), right_index=False)\n",
    "\n",
    "actual_items_id = train_df[train_df['date_block_num'] == train_df['date_block_num'].max()]['item_id']\n",
    "actual_items_id = np.isin(train_df['item_id'], actual_items_id)\n",
    "train_df['is_actual'] = np.where(actual_items_id, 1, 0).astype(np.int8)\n",
    "\n",
    "train_df['key'] = train_df.progress_apply(lambda x: str(int(x['shop_id'])) + \"_\" + str(int(x['item_id'])), axis=1)\n",
    "train_df['key'] = LabelEncoder().fit_transform(train_df['key']).astype(np.int32)\n",
    "\n",
    "train_df['year'] = (train_df['date_block_num'] // 12).astype(np.int8)\n",
    "train_df['month'] = (train_df['date_block_num'] % 12).astype(np.int8)\n",
    "\n",
    "#train_df_file_name = \"submission\" if does_it_for_submission else \"validation\"\n",
    "#train_df.to_pickle(f\"train_df_{train_df_file_name}_.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:30<00:00,  7.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'shop_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:15<00:00,  7.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'shop_id', 'item_category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id', 'item_category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:15<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "209.27204513549805"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "X_target_encoded = train_df\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id'], \n",
    "                                'item_cnt_month', 'date_item_avg_item_cnt', [1, 2, 3, 6])\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'shop_id'], \n",
    "                                 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6])\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_category_id'], \n",
    "                                 'item_cnt_month', 'date_cat_avg_item_cnt', [1, 6])\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'shop_id', 'item_category_id'], \n",
    "                                 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1, 6])\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1, 6])\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'city'], 'item_cnt_month', 'date_city_avg_item_cnt', [1, 6])\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id', 'city'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1, 6]) \n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix sales_train to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303.36906814575195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = sales_train.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(X_target_encoded, group, on=['item_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "group = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "lags = [1,2,3,4,5,6]\n",
    "matrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n",
    "\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0\n",
    "    \n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "matrix['delta_price_lag'].fillna(0, inplace=True)\n",
    "\n",
    "fetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    fetures_to_drop += ['delta_price_lag_'+str(i)]\n",
    "\n",
    "matrix.drop(fetures_to_drop, axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = matrix.drop('item_cnt_month', axis=1), matrix['item_cnt_month']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    X[X['date_block_num'] != predict_month], X[X['date_block_num'] == predict_month], \\\n",
    "    y[X['date_block_num'] != predict_month], y[X['date_block_num'] == predict_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (DataFrame)\n",
      "Stored 'X_test' (DataFrame)\n",
      "Stored 'y_train' (Series)\n",
      "Stored 'y_test' (Series)\n"
     ]
    }
   ],
   "source": [
    "if does_it_for_submission:\n",
    "    X_train_sub = X_train\n",
    "    X_test_sub = X_test\n",
    "    y_train_sub = y_train\n",
    "    y_test_sub = y_test\n",
    "    \n",
    "    %store X_train_sub\n",
    "    %store X_test_sub\n",
    "    %store y_train_sub\n",
    "    %store y_test_sub\n",
    "    \n",
    "    X_train_sub.to_pickle('X_train_sub.pkl')\n",
    "    X_test_sub.to_pickle('X_test_sub.pkl')\n",
    "    y_train_sub.to_pickle('y_train_sub.pkl')\n",
    "    y_test_sub.to_pickle('y_test_sub.pkl')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    %store X_train\n",
    "    %store X_test\n",
    "    %store y_train\n",
    "    %store y_test\n",
    "    \n",
    "    X_train.to_pickle('X_train.pkl')\n",
    "    X_test.to_pickle('X_test.pkl')\n",
    "    y_train.to_pickle('y_train.pkl')\n",
    "    y_test.to_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"26ee3a15-dbcd-4816-a0d2-04ef81aa7bd3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"26ee3a15-dbcd-4816-a0d2-04ef81aa7bd3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Kernel sales-prediction.feature_eng.python.2.0 executed successfuly\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Kernel sales-prediction.feature_eng.python.2.0 executed successfuly\"\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
