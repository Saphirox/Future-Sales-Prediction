{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. Shop features generation\n",
    "2. Shop features generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "does_it_for_submission = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyternotify\n",
    "\n",
    "%store -r item_cat\n",
    "%store -r item\n",
    "%store -r shops\n",
    "%store -r sales_train\n",
    "%store -r train\n",
    "%store -r train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper ipython script loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    body {\n",
       "          font-family: Helvetica, Times New Roman, sans-serif;\n",
       "    }\n",
       "    \n",
       "    h1,h2, h3,h4,h5,h6 {\n",
       "        font-family: Rockwell, Times New Roman, sans-serif;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksymsuprunenko/anaconda3/lib/python3.7/site-packages/tqdm/_tqdm.py:604: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Data Analysis tools was loaded\n"
     ]
    }
   ],
   "source": [
    "__da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maksymsuprunenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import googlemaps\n",
    "import plotly.express as px\n",
    "from functools import partial\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, KFold\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# TSFRESH\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, extract_features, MinimalFCParameters, EfficientFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import extract_relevant_features\n",
    "\n",
    "# Sklearn-pandas\n",
    "from sklearn_pandas import CategoricalImputer, FunctionTransformer, DataFrameMapper\n",
    "\n",
    "# SCIPY\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# My files\n",
    "from basic_text_preprocessing import BasicPreprocessText\n",
    "\n",
    "gmaps = googlemaps.Client(key='AIzaSyCW4PTjjIz6yGUgAmqrG2cLy9euzbim23M')\n",
    "\n",
    "from math import cos, asin, sqrt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shop features generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "1. **lat** - latitude\n",
    "2. **lng** - longitude\n",
    "3. **distance_to_moskov** - distance to Moscow city (Label Encoded)\n",
    "4. **city** - city (Label Encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295     #Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 12742 * np.arcsin(sqrt(a))\n",
    "\n",
    "def not_city_str(x, t):\n",
    "    return 1 if t in \"\".join(x.split()[1:]) else 0\n",
    "\n",
    "def get_location(x):\n",
    "    loc = gmaps.geocode(x)\n",
    "    return loc[0]['geometry']['location'] if len(loc) != 0 else {'lat': 0, 'lng': 0}\n",
    "\n",
    "moskov_lat, moskov_lng = get_location('Moscow')\n",
    "\n",
    "# new_shops = shops.copy()\n",
    "# cleaned_shop_name = BasicPreprocessText().vectorize_process_text(shops['shop_name'])\n",
    "# new_shops['shop_name'] = cleaned_shop_name\n",
    "# new_shops['city'] = new_shops['shop_name'].apply(lambda x: x.split()[0])\n",
    "# city = new_shops['city'] .value_counts()\\\n",
    "# .to_frame().reset_index().rename(columns={'index': 'shop_name', 'city': 'count_shops'})\n",
    "\n",
    "\n",
    "# new_shops['is_mal'] = new_shops['shop_name'].apply(partial(not_city_str, t='тц')).astype(np.int8)\n",
    "# new_shops['is_en_mal'] = new_shops['shop_name'].apply(partial(not_city_str, t='трк')).astype(np.int8)\n",
    "\n",
    "# locations = new_shops['shop_name'].progress_apply(get_location) \n",
    "\n",
    "# new_shops_with_coords = pd.concat([new_shops, pd.DataFrame.from_records(locations.values)], axis=1)\n",
    "\n",
    "# new_shops_with_coords.to_pickle(\"new_shops_with_coords.pickle\")\n",
    "\n",
    "new_shops_with_coords = pd.read_pickle(\"new_shops_with_coords.pickle\")\n",
    "moskov_lat, moskov_lng = list(get_location('Moscow').values())\n",
    "\n",
    "new_shops_with_coords['lat'] = new_shops_with_coords['lat'].astype(np.float16, copy=False)\n",
    "new_shops_with_coords['lng'] = new_shops_with_coords['lng'].astype(np.float16, copy=False)\n",
    "\n",
    "new_shops_with_coords['distance_to_moskov'] = \\\n",
    "    new_shops_with_coords[['lat', 'lng']].apply(lambda x: distance(x[0], x[1], moskov_lat, moskov_lng), axis=1)\\\n",
    "    .astype(np.float16)\n",
    "\n",
    "le_shop_dtm = LabelEncoder().fit(new_shops_with_coords['distance_to_moskov'].sort_values().values)\n",
    "\n",
    "new_shops_with_coords['distance_to_moskov'] = \\\n",
    "    le_shop_dtm.transform(new_shops_with_coords['distance_to_moskov']).astype(np.float16)\n",
    "\n",
    "new_shops_with_coords['city'] = LabelEncoder().fit_transform(new_shops_with_coords['city']).astype(np.int8)\n",
    "\n",
    "new_shops_with_coords = new_shops_with_coords.drop('shop_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove item name from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"1f9cceba-1418-4d6b-894c-661c2caf30dc\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"1f9cceba-1418-4d6b-894c-661c2caf30dc\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "item_cleaned = item.copy()\n",
    "item_cleaned = item_cleaned.drop('item_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item category generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "1. item_category_names_category_1_enc\n",
    "2. item_category_names_category_2_enc\n",
    "3. is_digital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat_cleaned = item_cat.copy()\n",
    "\n",
    "item_category_names = pd.Series(\n",
    "    BasicPreprocessText().vectorize_process_text(item_cat_cleaned['item_category_name'], ['-'])\n",
    ")\n",
    "\n",
    "idx = [8, 9, 32, 79, 80, 81, 82, 83]\n",
    "fixed_first_level = item_category_names[np.isin(item_cat_cleaned.index, idx)].apply(lambda x: str(np.abs(np.random.normal())) + \"-\" + x)\n",
    "item_category_names[idx] = fixed_first_level\n",
    "\n",
    "item_cat_cleaned['item_category_names_category_1'] = item_category_names.apply(lambda x: x.split(\"-\")[0])\n",
    "item_cat_cleaned['item_category_names_category_2'] = item_category_names.apply(lambda x: \" \".join(x.split(\"-\")[1:]))\n",
    "\n",
    "item_cat_cleaned['item_category_names_category_1_enc'] = \\\n",
    "    LabelEncoder().fit_transform(item_cat_cleaned['item_category_names_category_1']).astype(np.int8)\n",
    "\n",
    "item_cat_cleaned['item_category_names_category_2_enc'] = \\\n",
    "    LabelEncoder().fit_transform(item_cat_cleaned['item_category_names_category_2']).astype(np.int8)\n",
    "\n",
    "item_cat_cleaned['is_digital'] = item_cat_cleaned.apply(lambda x: 'цыфра' in x).astype(np.int8)\n",
    "\n",
    "item_cat_cleaned = item_cat_cleaned.drop(\n",
    "    ['item_category_name', 'item_category_names_category_1', 'item_category_names_category_2'], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join on everthing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join tables:\n",
    "1. item\n",
    "2. item_categories\n",
    "3. sales_train\n",
    "4. shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"7526f1e5-b0d1-49c5-aa35-d374ab64a900\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"7526f1e5-b0d1-49c5-aa35-d374ab64a900\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "\n",
    "predict_month = 34 if does_it_for_submission else 33\n",
    "train_df = train_test.copy() if does_it_for_submission else train.copy()\n",
    "\n",
    "train_df = train_df.merge(item_cleaned[['item_id', 'item_category_id']], how='left', on='item_id', suffixes=(\"\", \"_item\"), right_index=False)\n",
    "train_df = train_df.merge(new_shops_with_coords, how='left', on='shop_id', suffixes=(\"\", \"_shops\"), right_index=False)\n",
    "train_df = train_df.merge(item_cat_cleaned, how='left', on='item_category_id', suffixes=(\"\", \"_item_cat\"), right_index=False)\n",
    "\n",
    "#train_df_file_name = \"submission\" if does_it_for_submission else \"validation\"\n",
    "#train_df.to_pickle(f\"train_df_{train_df_file_name}_.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General features\n",
    "1. key - compound key of shop_id and item_id\n",
    "2. year \n",
    "3. month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11128050/11128050 [03:43<00:00, 49807.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['key'] = train_df.progress_apply(lambda x: str(int(x['shop_id'])) + \"_\" + str(int(x['item_id'])), axis=1)\n",
    "train_df['key'] = LabelEncoder().fit_transform(train_df['key']).astype(np.int32)\n",
    "\n",
    "train_df['year'] = (train_df['date_block_num'] // 12).astype(np.int8)\n",
    "train_df['month'] = (train_df['date_block_num'] % 12).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group sale stats in recent\n",
    "create stats (mean/var) of sales of certain groups during the past 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n",
    "    if not 'date_block_num' in groupby_feats:\n",
    "        print ('date_block_num must in groupby_feats')\n",
    "        return matrix_\n",
    "    \n",
    "    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n",
    "    max_lags = np.max(last_periods)\n",
    "    \n",
    "    for i in tqdm(range(1, max_lags+1)):\n",
    "        shifted = group[groupby_feats+[target]].copy(deep=True)\n",
    "        shifted['date_block_num'] += i\n",
    "        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n",
    "        group = group.merge(shifted, on=groupby_feats, how='left')\n",
    "    group.fillna(0,inplace=True)\n",
    "    \n",
    "    for period in tqdm(last_periods):\n",
    "        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n",
    "        # we do not use mean and std directly because we want to include months with sales = 0\n",
    "        mean = group[lag_feats].sum(axis=1)/float(period)\n",
    "        mean2 = (group[lag_feats]**2).sum(axis=1)/float(period)\n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n",
    "        # divide by mean, this scales the features for NN\n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] /= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] /= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n",
    "        \n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] = group[enc_feat+'_avg_sale_last_'+str(period)].astype(np.float16) \n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] = group[enc_feat+'_std_sale_last_'+str(period)].astype(np.float16)\n",
    "        \n",
    "        group[enc_feat+'_min_sale_last_'+str(period)] = group[lag_feats].sum(axis=1).min()\n",
    "        group[enc_feat+'_max_sale_last_'+str(period)] = group[lag_feats].sum(axis=1).max()\n",
    "        \n",
    "        group[enc_feat+'_min_sale_last_'+str(period)] = group[enc_feat+'_min_sale_last_'+str(period)].astype(np.float16)\n",
    "        group[enc_feat+'_max_sale_last_'+str(period)] = group[enc_feat+'_max_sale_last_'+str(period)].astype(np.float16)\n",
    "                \n",
    "    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n",
    "    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 13.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.86it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 164.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 81.57it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 120.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 70.83it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 155.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 67.25it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 215.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 60.00it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 120.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.28959584236145"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "X_target_encoded = train_df\n",
    "\n",
    "X_target_encoded = add_group_stats(X_target_encoded, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\n",
    "X_target_encoded = add_group_stats(X_target_encoded, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\n",
    "X_target_encoded = add_group_stats(X_target_encoded, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\n",
    "X_target_encoded = add_group_stats(X_target_encoded, ['date_block_num', 'city'], 'item_cnt_month', 'city', [12])\n",
    "X_target_encoded = add_group_stats(X_target_encoded, ['date_block_num', 'item_category_names_category_1_enc'], 'item_cnt_month', 'family', [12])\n",
    "X_target_encoded = add_group_stats(X_target_encoded, ['date_block_num', 'item_category_names_category_2_enc'], 'item_cnt_month', 'subfamily', [12])\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:43<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:47<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'shop_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:49<00:00, 10.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:59<00:00, 11.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'shop_id', 'item_category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:03<00:00, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id', 'item_category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:08<00:00, 13.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:14<00:00, 14.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:44<00:00, 22.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'shop_id', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:47<00:00, 24.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['date_block_num', 'item_id', 'item_category_names_category_1_enc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:43<00:00, 21.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "805.8235862255096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lag_feature(df, lags, col):    \n",
    "    tmp = df[['date_block_num', 'shop_id','item_id', col]]\n",
    "    for i in tqdm(lags):\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] = shifted['date_block_num'] + i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df\n",
    "\n",
    "def mean_encoding(df, groupby_feats, target, enc, lags):\n",
    "    print('Features: ' , groupby_feats)\n",
    "    features = df[[*groupby_feats, target]]\\\n",
    "             .groupby(groupby_feats, as_index=False)\\\n",
    "             .agg(['mean'])\n",
    "   \n",
    "    features.columns = [enc]\n",
    "    \n",
    "    df = df.merge(features, on=groupby_feats, how='left')\n",
    "    df[enc] = df[enc].astype(np.float16)\n",
    "    df = lag_feature(df, lags, enc).fillna(0)\n",
    "    df.drop(enc, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "periods = [1, 2, 3, 6, 12]\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num'], 'item_cnt_month', \n",
    "                                 'date_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id'], \n",
    "                                'item_cnt_month', 'date_item_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'shop_id'], \n",
    "                                 'item_cnt_month', 'date_shop_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_category_id'], \n",
    "                                 'item_cnt_month', 'date_cat_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'shop_id', 'item_category_id'], \n",
    "                                 'item_cnt_month', 'date_shop_cat_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id', 'item_category_id'], \n",
    "                                 'item_cnt_month', \n",
    "                                 'date_item_id_cat_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'city'], \n",
    "                                 'item_cnt_month', 'date_city_avg_item_cnt', periods)\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id', 'city'], \n",
    "                                 'item_cnt_month', 'date_item_city_avg_item_cnt', [1, 6]) \n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'shop_id', 'city'], \n",
    "                                 'item_cnt_month', 'date_shop_city_avg_item_cnt', [1, 6])\n",
    "\n",
    "X_target_encoded = mean_encoding(X_target_encoded, ['date_block_num', 'item_id', \n",
    "                                                    'item_category_names_category_1_enc'], \n",
    "                                 'item_cnt_month', 'date_item_category_1_avg_item_cnt', [1, 6])\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11128050 entries, 0 to 11128049\n",
      "Data columns (total 92 columns):\n",
      "date_block_num                             int8\n",
      "shop_id                                    int8\n",
      "item_id                                    int16\n",
      "item_cnt_month                             float32\n",
      "item_price                                 float32\n",
      "item_category_id                           int64\n",
      "city                                       int8\n",
      "is_mal                                     int8\n",
      "is_en_mal                                  int8\n",
      "lat                                        float16\n",
      "lng                                        float16\n",
      "distance_to_moskov                         float16\n",
      "item_category_names_category_1_enc         int8\n",
      "item_category_names_category_2_enc         int8\n",
      "is_digital                                 float64\n",
      "key                                        int32\n",
      "year                                       int8\n",
      "month                                      int8\n",
      "revenue                                    float32\n",
      "item_avg_sale_last_6                       float16\n",
      "item_std_sale_last_6                       float16\n",
      "item_min_sale_last_6                       float16\n",
      "item_max_sale_last_6                       float16\n",
      "item_avg_sale_last_12                      float16\n",
      "item_std_sale_last_12                      float16\n",
      "item_min_sale_last_12                      float16\n",
      "item_max_sale_last_12                      float16\n",
      "shop_avg_sale_last_6                       float16\n",
      "shop_std_sale_last_6                       float16\n",
      "shop_min_sale_last_6                       float16\n",
      "shop_max_sale_last_6                       float16\n",
      "shop_avg_sale_last_12                      float16\n",
      "shop_std_sale_last_12                      float16\n",
      "shop_min_sale_last_12                      float16\n",
      "shop_max_sale_last_12                      float16\n",
      "category_avg_sale_last_12                  float16\n",
      "category_std_sale_last_12                  float16\n",
      "category_min_sale_last_12                  float16\n",
      "category_max_sale_last_12                  float16\n",
      "city_avg_sale_last_12                      float16\n",
      "city_std_sale_last_12                      float16\n",
      "city_min_sale_last_12                      float16\n",
      "city_max_sale_last_12                      float16\n",
      "family_avg_sale_last_12                    float16\n",
      "family_std_sale_last_12                    float16\n",
      "family_min_sale_last_12                    float16\n",
      "family_max_sale_last_12                    float16\n",
      "subfamily_avg_sale_last_12                 float16\n",
      "subfamily_std_sale_last_12                 float16\n",
      "subfamily_min_sale_last_12                 float16\n",
      "subfamily_max_sale_last_12                 float16\n",
      "date_avg_item_cnt_lag_1                    float16\n",
      "date_avg_item_cnt_lag_2                    float16\n",
      "date_avg_item_cnt_lag_3                    float16\n",
      "date_avg_item_cnt_lag_6                    float16\n",
      "date_avg_item_cnt_lag_12                   float16\n",
      "date_item_avg_item_cnt_lag_1               float16\n",
      "date_item_avg_item_cnt_lag_2               float16\n",
      "date_item_avg_item_cnt_lag_3               float16\n",
      "date_item_avg_item_cnt_lag_6               float16\n",
      "date_item_avg_item_cnt_lag_12              float16\n",
      "date_shop_avg_item_cnt_lag_1               float16\n",
      "date_shop_avg_item_cnt_lag_2               float16\n",
      "date_shop_avg_item_cnt_lag_3               float16\n",
      "date_shop_avg_item_cnt_lag_6               float16\n",
      "date_shop_avg_item_cnt_lag_12              float16\n",
      "date_cat_avg_item_cnt_lag_1                float16\n",
      "date_cat_avg_item_cnt_lag_2                float16\n",
      "date_cat_avg_item_cnt_lag_3                float16\n",
      "date_cat_avg_item_cnt_lag_6                float16\n",
      "date_cat_avg_item_cnt_lag_12               float16\n",
      "date_shop_cat_avg_item_cnt_lag_1           float16\n",
      "date_shop_cat_avg_item_cnt_lag_2           float16\n",
      "date_shop_cat_avg_item_cnt_lag_3           float16\n",
      "date_shop_cat_avg_item_cnt_lag_6           float16\n",
      "date_shop_cat_avg_item_cnt_lag_12          float16\n",
      "date_item_id_cat_avg_item_cnt_lag_1        float16\n",
      "date_item_id_cat_avg_item_cnt_lag_2        float16\n",
      "date_item_id_cat_avg_item_cnt_lag_3        float16\n",
      "date_item_id_cat_avg_item_cnt_lag_6        float16\n",
      "date_item_id_cat_avg_item_cnt_lag_12       float16\n",
      "date_city_avg_item_cnt_lag_1               float16\n",
      "date_city_avg_item_cnt_lag_2               float16\n",
      "date_city_avg_item_cnt_lag_3               float16\n",
      "date_city_avg_item_cnt_lag_6               float16\n",
      "date_city_avg_item_cnt_lag_12              float16\n",
      "date_item_city_avg_item_cnt_lag_1          float16\n",
      "date_item_city_avg_item_cnt_lag_6          float16\n",
      "date_shop_city_avg_item_cnt_lag_1          float16\n",
      "date_shop_city_avg_item_cnt_lag_6          float16\n",
      "date_item_category_1_avg_item_cnt_lag_1    float16\n",
      "date_item_category_1_avg_item_cnt_lag_6    float16\n",
      "dtypes: float16(76), float32(3), float64(1), int16(1), int32(1), int64(1), int8(9)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "X_target_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix sales_train to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:43<00:00, 17.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "507.49088978767395"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = sales_train.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(X_target_encoded, group, on=['item_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "group = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "lags = [1,2,3,4,5,6]\n",
    "matrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n",
    "\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0\n",
    "    \n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "matrix['delta_price_lag'].fillna(0, inplace=True)\n",
    "\n",
    "fetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    fetures_to_drop += ['delta_price_lag_'+str(i)]\n",
    "\n",
    "matrix.drop(fetures_to_drop, axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column 'revenue' does not exist!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-89dfc88435b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_block_num'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'shop_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'revenue'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date_shop_revenue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aggregate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m                         \u001b[0mnested_renaming_depr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column '{col}' does not exist!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column 'revenue' does not exist!\""
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\n",
    "group.columns = ['date_shop_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
    "\n",
    "group = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\n",
    "group.columns = ['shop_avg_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['shop_id'], how='left')\n",
    "matrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n",
    "\n",
    "matrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
    "matrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], 'delta_revenue')\n",
    "\n",
    "matrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.19337439537048"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Month since last sale for each shop/item pair.\n",
    "ts = time.time()\n",
    "last_sale = pd.DataFrame()\n",
    "for month in range(1,35):    \n",
    "    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n",
    "    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n",
    "                       'item_id': last_month.index.get_level_values(0).values,\n",
    "                       'shop_id': last_month.index.get_level_values(1).values,\n",
    "                       'item_shop_last_sale': last_month.values})\n",
    "    last_sale = last_sale.append(df)\n",
    "last_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n",
    "\n",
    "matrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.104480028152466"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Month since last sale for each item.\n",
    "ts = time.time()\n",
    "last_sale = pd.DataFrame()\n",
    "for month in range(1,35):    \n",
    "    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n",
    "    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n",
    "                       'item_id': last_month.index.values,\n",
    "                       'item_last_sale': last_month.values})\n",
    "    last_sale = last_sale.append(df)\n",
    "last_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n",
    "\n",
    "matrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.262052059173584"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Months since the first sale for each shop/item pair and for item only.\n",
    "ts = time.time()\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = matrix.drop('item_cnt_month', axis=1), matrix['item_cnt_month']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    X[X['date_block_num'] != predict_month], X[X['date_block_num'] == predict_month], \\\n",
    "    y[X['date_block_num'] != predict_month], y[X['date_block_num'] == predict_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train_sub' (DataFrame)\n",
      "Stored 'X_test_sub' (DataFrame)\n",
      "Stored 'y_train_sub' (Series)\n",
      "Stored 'y_test_sub' (Series)\n"
     ]
    }
   ],
   "source": [
    "if does_it_for_submission:\n",
    "    X_train_sub = X_train\n",
    "    X_test_sub = X_test\n",
    "    y_train_sub = y_train\n",
    "    y_test_sub = y_test\n",
    "    \n",
    "    %store X_train_sub\n",
    "    %store X_test_sub\n",
    "    %store y_train_sub\n",
    "    %store y_test_sub\n",
    "    \n",
    "    X_train_sub.to_pickle('X_train_sub.pkl')\n",
    "    X_test_sub.to_pickle('X_test_sub.pkl')\n",
    "    y_train_sub.to_pickle('y_train_sub.pkl')\n",
    "    y_test_sub.to_pickle('y_test_sub.pkl')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    %store X_train\n",
    "    %store X_test\n",
    "    %store y_train\n",
    "    %store y_test\n",
    "    \n",
    "    X_train.to_pickle('X_train.pkl')\n",
    "    X_test.to_pickle('X_test.pkl')\n",
    "    y_train.to_pickle('y_train.pkl')\n",
    "    y_test.to_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify -m \"Kernel sales-prediction.feature_eng.python.2.0 executed successfuly\"\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
